<!DOCTYPE html>

<html>
	<head>
	    <meta charset="utf-8">
		<link rel="stylesheet" href="../common-revealjs/css/reveal.css">
		<link rel="stylesheet" href="../common-revealjs/css/theme/white.css">
		<link rel="stylesheet" href="../common-revealjs/css/custom.css">
		<script>
			// This is needed when printing the slides to pdf
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
		<script>
		    // This is used to display the static images on each slide,
			// See global-images in this html file and custom.css
			(function() {
				if(window.addEventListener) {
					window.addEventListener('load', () => {
						let slides = document.getElementsByClassName("slide-background");

						if (slides.length === 0) {
							slides = document.getElementsByClassName("pdf-page")
						}

						// Insert global images on each slide
						for(let i = 0, max = slides.length; i < max; i++) {
							let cln = document.getElementById("global-images").cloneNode(true);
							cln.removeAttribute("id");
							slides[i].appendChild(cln);
						}

						// Remove top level global images
						let elem = document.getElementById("global-images");
						elem.parentElement.removeChild(elem);
					}, false);
				}
			})();
		</script>
		
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<div id="global-images" class="global-images">
					<img src="../common-revealjs/images/sycl_academy.png" />
					<img src="../common-revealjs/images/sycl_logo.png" />
					<img src="../common-revealjs/images/trademarks.png" />
					<img src="../common-revealjs/images/codeplay.png" />
				</div>
				<!--Slide 1-->
				<section class="hbox" data-markdown>
					## SYCL Kernel Functions
				</section>
				<!--Slide 2-->
				<section class="hbox" data-markdown>
					## Learning Objectives

					* The SYCL execution model
					* The SIMT model for describing parallelism
					* Define and invoke SYCL kernel functions
					* Rules and restrictions on kernel functions
					* Using both lambdas and function objects
					* Manual compilation of kernel functions
				</section>
				<!--Slide 3-->
				<section class="hbox" data-markdown>
				    ## The SYCL Execution Model
				</section>
				<!--Slide 4-->
				<section>
					<div class="container">
						<div class="col-left" data-markdown>
							* SYCL kernel functions are executed by **work-items**
							* You can think of a work-item as a thread of execution
							* Each work-item will execute a SYCL kernel function from start to end
							* A work-item can run on CPU threads, SIMD lanes, GPU threads, or any other kind of processing element
						</div>
						<div class="col-right" data-markdown>
							![Work Item](../common-revealjs/images/workitem.png "Work-Item")
						</div>

					</div>
				</section>
				<!--Slide 5-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* Work-items are collected together into **work-groups**
							* The size of work-groups is generally relative to what is optimal on the device being targeted
							* It can also be affected by the resources used by each work-item
						</div>
						<div class="col" data-markdown>
							![Work Group](../common-revealjs/images/workgroup.png "Work-Group")
						</div>
					</div>
				</section>
				<!--Slide 6-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* SYCL kernel functions are invoked within an **nd-range**
							* An nd-range has a number of work-groups and subsequently a number of work-items
							* Work-groups always have the same number of work-items
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/ndrange.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 7-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* The nd-range describes an **iteration space**; how the work-items and work-groups are composed
							* An nd-range can be 1, 2 or 3 dimensions
							* An nd-range has two components
							  * The **global-range** describes the total number of workitems in each dimension
							  * The **local-range** describes the number of work-items in a work-group in each dimension
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/ndrange-example.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 8-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* Each invocation in the iteration space of an nd-range is a work-item
							* Each invocation knows which work-item it is on and can query certain information about its position in the nd-range
							* Each work-item has the following:
							  * **Global range**: {12, 12}
							  * **Global id**: {6, 5}
							  * **Group range**: {3, 3}
							  * **Group id**: {1, 1}
							  * **Local range**: {4, 4}
							  * **Local id**: {2, 1}
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/ndrange-example-work-item.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 9-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							Typically an nd-range invocation SYCL will execute the SYCL kernel function on a very large number of work-items, often in the thousands
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/ndrange-invocation.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 10-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* Multiple work-items will generally execute concurrently
							* On vector hardware this is often done in lock-step, which means the same hardware instructions
							* The number of work-items that will execute concurrently can vary from one device to another
							* Work-items will be batched along with other work-items in the same work-group
							* The order work-items and workgroups are executed in is implementation defined
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/ndrange-lock-step.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 11-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* Work-items in a work-group can be synchronized using a work-group barrier
							  * All work-items within a work-group must reach the barrier before any can continue on
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/work-group-0.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 12-->
				<section>
					<div class="container">
						<div class="col" data-markdown>
							* SYCL does not support synchronizing across all work-items in the nd-range
							* The only way to do this is to split the computation into separate SYCL kernel functions
						</div>
						<div class="col" data-markdown>
							![ND-Range](../common-revealjs/images/work-group-0-1.png "ND-Range")
						</div>
					</div>
				</section>
				<!--Slide 13-->
				<section class="hbox" data-markdown>
					## SYCL is a SIMT Programming Model
				</section>
				<!--Slide 14-->
				<section>
					<div class="container">
						<div class="col">
							Sequential CPU code
							<code><pre>
void calc(int *in, int *out) {
  // all iterations are run in the same 
  // thread in a loop 
  for (int i = 0; i < 1024; i++){ 
    out[i] = in[i] * in[i];
  } 
}

// calc is invoked just once and all 
// iterations are performed inline 
calc(in, out);
							</code></pre>
						</div>
						<div class="col">
							Parallel SIMT code
							<code><pre>
void calc(int *in, int *out, int id) { 
  // function is described in terms of 
  // a single iteration 
  out[id] = in[id] * in[id];
}

// parallel_for invokes calc multiple 
// times in parallel 
parallel_for(calc, in, out, 1024);


							</code></pre>
						</div>
					</div>
				</section>
				<!--Slide 15-->
				<section class="hbox" data-markdown>
					## Enqueueing SYCL Kernel Functions
				</section>
				<!--Slide 16-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
#include &ltCL/sycl.hpp&gt 
using namespace cl::sycl;

class add;

int main(int argc, char *argv[]) { 
  queue q(gpu_selector{});

  q.submit([&](handler &cgh){

    cgh.parallel_for&ltadd&gt(range&lt1&gt(1024), 
      [=](id&lt1&gt i) { 
      // kernel code }); 
  }); 
  q.wait();
}

							</code></pre>
						</div>
						<div class="col" data-markdown>
							* SYCL kernel functions are defined and invoked using one of the kernel function invoke APIs provided by the **handler** class
							* These add a SYCL kernel function command to the command group
							* There can only be one SYCL kernel function command in a command group
						</div>
					</div>
				</section>
				<!--Slide 17-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
#include &ltCL/sycl.hpp&gt 
using namespace cl::sycl;

class add;

int main(int argc, char *argv[]) { 
  queue q(gpu_selector{});

  q.submit([&](handler &cgh){

    cgh.parallel_for&ltadd&gt(range&lt1&gt(1024), 
      <mark>[=](id&lt1&gt i) { 
      // kernel code }); 
  }); </mark>
  q.wait();
}

							</code></pre>
						</div>
						<div class="col" data-markdown>
							* The lambda or function object represents the SYCL device function
							* This is the part that is compiled for the device
						</div>
					</div>
				</section>
				<!--Slide 18-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
#include &ltCL/sycl.hpp&gt 
using namespace cl::sycl;

class add;

int main(int argc, char *argv[]) { 
  queue q(gpu_selector{});

  q.submit([&](handler &cgh){

    cgh.parallel_for&ltadd&gt(<mark>range&lt1&gt(1024), 
      [=](id&lt1&gt i</mark>) { 
      // kernel code }); 
  }); 
  q.wait();
}
							</code></pre>
						</div>
						<div class="col" data-markdown>
							* There are a number of different APIs for expressing different forms of parallelism, complexity and functionality
							* Each takes some representation of the nd-range and expects a certain parameter type that describes the current index into the iteration space
							* These types have a number of member functions for retrieving different index and range information about the current iteration
						</div>
					</div>
				</section>
				<!--Slide 19-->
				<section>
					Expressing Parallelism
					<div class="container">
						<div class="col">
							<code><pre>
cgh.<mark>single_task</mark><T>([=](){
  // SYCL kernel function is executed 
  // once on a single work-item
});
							</code></pre>
							<code><pre>
cgh.<mark>parallel_for</mark><T>(range&lt2&gt(64, 64), 
                          [=](id&lt2&gt idx){
  // SYCL kernel function is executed 
  // on an nd-range of work-items
});
							</code></pre>
						</div>
						<div class="col">
							<code><pre>
cgh.<mark>parallel_for_work_group</mark>(range&lt2&gt(64, 64), 
                     [=](group&lt2&gt gp){
  // SYCL kernel function is executed  
  // once per work-group
							</code></pre>
							<code><pre>
  <mark>parallel_for_work_item</mark>(gp, [=](h_item&lt2&gt it){
    // SYCL kernel function is executed  
    // once per work-item
  });
});
							</code></pre>
						</div>
					</div>
				</section>
				<!--Slide 20-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(<mark>range&lt1&gt(1024)</mark>, 
    [=](<mark>id&lt1&gt idx</mark>){

  // kernel code

});

							</code></pre>
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(<mark>range&lt1&gt(1024)</mark>, 
    [=](<mark>item&lt1&gt item</mark>){

  // kernel code

});

							</code></pre>
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(nd_range&lt1&gt(<mark>range&lt1&gt(1024), 
    range&lt1&gt(32))</mark>,[=](<mark>nd_item&lt1&gt ndItem</mark>){

  // kernel code

});

							</code></pre>
						</div>
						<div class="col" data-markdown>
							* Overload taking a **range** object specifies the global range, runtime decides local range
							* An **id** parameter represents the index within the global range
							____________________________________________________________________________________________
							* Overload taking a **range** object specifies the global range, runtime decides local range
							* An **item** parameter represents the global range and the index within the global range
							____________________________________________________________________________________________
							* Overload taking an **nd_range** object specifies the global and local range
							* An **nd_item** parameter represents the global and local range and index
						</div>
					</div>
				</section>
				<!--Slide 21-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(range&lt1&gt(1024), 
    [=](<mark>id&lt1&gt(512)</mark>, id&lt1&gtidx){

  // kernel code

});

							</code></pre>
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(range&lt1&gt(1024), 
     <mark>id&lt1&gt(512)</mark>, [=](item&lt1&gt item){
      
  // kernel code

});

							</code></pre>
							<code><pre>
							
cgh.parallel_for&ltkernel&gt(nd_range&lt1&gt(range&lt1&gt(1024), 
    range&lt1&gt(32), <mark>id&lt1&gt(512)</mark>),
    [=](nd_item&lt1&gt ndItem){

  // kernel code

});

							</code></pre>
						</div>
						<div class="col" data-markdown>
							* All overloads of **parallel_for** also allow you to optionally specify an offset
							* The offset, if used, will increment each index into the global index by the specified value
							  * E.g. with a range of 1024 and an offset of 512 the indexes would become (512, 1536)
						</div>
					</div>
				</section>
				<!--Slide 22-->
				<section class="hbox" data-markdown>
					## SYCL kernel function rules and restrictions
				</section>
				<!--Slide 23-->
				<section>
					SYCL Kernel Function Rules
					<div class=container data-markdown>
						* Must be defined using a C++ lambda or function object, they cannot be a function pointer or std::function
						* Must always capture or store members by-value
						* SYCL kernel functions declared with a lambda must be named using a forward declarable C++ type, declared in global scope 
						* SYCL kernel function names follow C++ ODR rules, which means you cannot have two kernels with the same name
					</div>
				</section>
				<!--Slide 24-->
				<section>
					SYCL Kernel Function Restrictions
					<div class=container data-markdown>
						* No dynamic allocation
						* No dynamic polymorphism
						* No function pointers
						* No recursion
					</div>
				</section>
				<!--Slide 25-->
				<section class="hbox" data-markdown>
					## SYCL kernels as function objects
				</section>
				<!--Slide 26-->
				<section>
					<div class="container">
						<div class="col-left-3">
							<code><pre>
#include &ltCL/sycl.hpp&gt
using namespace cl::sycl;
class add;

int main(int argc, char *argv[]) {
  std::vector&ltfloat&gt dA{ … }, dB{ … }, dO{ … };
  try { 
    queue q(gpu_selector{}, 
      async_handler{});
    buffer&ltfloat, 1&gt bufA(dA.data(), range&lt1&gt(dA.size()));
    buffer&ltfloat, 1&gt bufB(dB.data(), range&lt1&gt(dB.size()));
    buffer&ltfloat, 1&gt bufO(dO.data(), range&lt1&gt(dO.size()));
    q.submit([&](handler &cgh){

      auto inA = bufA.get_access&ltaccess::mode::read&gt(cgh);
      auto inB = bufB.get_access&ltaccess::mode::read&gt(cgh);
      auto out = bufO.get_access&ltaccess::mode::write&gt(cgh);

      cgh.parallel_for&ltadd&gt(range&lt1&gt(dA.size()),
        <mark>[=](id&lt1&gt i){ out[i] = inA[i] + inB[i]; }</mark>);
    }); 
    q.wait_and_throw();
  } catch (...) { /* handle errors */ }
}
							</code></pre>
						</div>
						<div class="col-right-1" data-markdown>
							* All the examples of SYCL kernel functions up until now have been defined using lambdas
						</div>
					</div>
				</section>
				<!--Slide 27-->
				<section>
					<div class="container">
						<div class="col">
							<code><pre>
struct add { 
  using read_accessor_t = 
    accessor&ltfloat, 1, 
    access::mode::read, 
    access::target::global_buffer&gt;
  using write_accessor_t = 
    accessor&ltfloat, 1 
    access::mode::write, 
    access::target::global_buffer&gt;
    
  read_accessor_t inA_, inB_; 
  write_accessor_t out_;

void operator()(id&lt1&gt i){ 
  out_[i] = inA_[i] + inB_[i]; 
  }
};
							</code></pre>
						</div>
						<div class="col" data-markdown>
							* As well as defining SYCL kernels using lambdas you can also define a SYCL kernel using a regular C++ function object
							* Where the accessors are stored as members and the function call operator takes the appropriate parameter
						</div>
					</div>
				</section>
				<!--Slide 28-->
				<section>
					<div class="container">
						<div class="col-left-3">
							<code><pre>
#include &ltCL/sycl.hpp&gt
using namespace cl::sycl;
class add;

int main(int argc, char *argv[]) {
  std::vector&ltfloat&gt dA{ … }, dB{ … }, dO{ … };
  try { 
    queue q(gpu_selector{}, 
      async_handler{});
    buffer&ltfloat, 1&gt bufA(dA.data(), range&lt1&gt(dA.size()));
    buffer&ltfloat, 1&gt bufB(dB.data(), range&lt1&gt(dB.size()));
    buffer&ltfloat, 1&gt bufO(dO.data(), range&lt1&gt(dO.size()));
    q.submit([&](handler &cgh){

      auto inA = bufA.get_access&ltaccess::mode::read&gt(cgh);
      auto inB = bufB.get_access&ltaccess::mode::read&gt(cgh);
      auto out = bufO.get_access&ltaccess::mode::write&gt(cgh);

      <mark>cgh.parallel_for(range&lt1&gt(dA.size()), 
        add{inA, inB, out});</mark>
    }); 
    q.wait_and_throw();
  } catch (...) { /* handle errors */ }
}
							</code></pre>
						</div>
						<div class="col-right-1">
						<code><pre>
struct add { 
  using read_accessor_t = 
    accessor&ltfloat, 1, 
    access::mode::read, 
    access::target::global_buffer&gt; 
  using write_accessor_t = 
    accessor&ltfloat, 1 
    access::mode::write, 
    access::target::global_buffer&lt;
  
  read_accessor_t inA_, inB_; 
  write_accessor_t out_;

  void operator()(id&lt1&gt i) ){ 
  out[i] = inA[i] + inB[i]; 
  } 
};
						</code></pre>
						
							To use a C++ function object you simply construct an instance of the type initialising the accessors and pass it to parallel_for<br><br>
							
							Notice you no longer need to name the SYCL kernel
						</div>
					</div>
				</section>
				<!--Slide 29-->
				<section class="hbox" data-markdown>
				## Pre-Compiling SYCL Kernel Functions
				</section>
				<!--Slide 30-->
				<section>
										<div class="container">
						<div class="col-left-3">
							<code><pre>
#include &ltCL/sycl.hpp&gt
using namespace cl::sycl;
class add;

int main(int argc, char *argv[]) {
  std::vector&ltfloat&gt dA{ … }, dB{ … }, dO{ … };
  try { 
    queue q(gpu_selector{}, 
      async_handler{});
    buffer&ltfloat, 1&gt bufA(dA.data(), range&lt1&gt(dA.size()));
    buffer&ltfloat, 1&gt bufB(dB.data(), range&lt1&gt(dB.size()));
    buffer&ltfloat, 1&gt bufO(dO.data(), range&lt1&gt(dO.size()));
    q.submit([&](handler &cgh){

      auto inA = bufA.get_access&ltaccess::mode::read&gt(cgh);
      auto inB = bufB.get_access&ltaccess::mode::read&gt(cgh);
      auto out = bufO.get_access&ltaccess::mode::write&gt(cgh);

      cgh.parallel_for(range&lt1&gt(dA.size()), 
        add{inA, inB, out});
    }); 
    q.wait_and_throw();
  } catch (...) { /* handle errors */ }
}
							</code></pre>
						</div>
						<div class="col-right-2" data-markdown>
							* When you emqueue a SYCL kernel function the runtime has to just-in-time compile the kernel for the device that the queue is targeting
							* This means the first time a kernel function is enqueued will take longer
							* However you can avoid this by pre-compiling the kernel
							* Pre-compiling kernels also gives you more control over how the kernel is compiled
						</div>
					</div>
				</section>
				<!--Slide 31-->
				<section>
										<div class="container">
						<div class="col-left-3">
							<code><pre>
int main(int argc, char *argv[]) {
  std::vector&ltfloat&gt dA{ … }, dB{ … }, dO{ … };
  try { 
    queue q(gpu_selector{}, 
      async_handler{});

    <mark>program addProgram(q.get_context());</mark>

    buffer&ltfloat, 1&gt bufA(dA.data(), range&lt1&gt(dA.size()));
    buffer&ltfloat, 1&gt bufB(dB.data(), range&lt1&gt(dB.size()));
    buffer&ltfloat, 1&gt bufO(dO.data(), range&lt1&gt(dO.size()));
    q.submit([&](handler &cgh){

      auto inA = bufA.get_access&ltaccess::mode::read&gt(cgh);
      auto inB = bufB.get_access&ltaccess::mode::read&gt(cgh);
      auto out = bufO.get_access&ltaccess::mode::write&gt(cgh);

      cgh.parallel_for(range&lt1&gt(dA.size()), 
        add{inA, inB, out});
    }); 
    q.wait_and_throw();
  } catch (...) { /* handle errors */ }
}
							</code></pre>
						</div>
						<div class="col-right-2" data-markdown>
							* To pre-compile a kernel you need to create a program that is associated with the context you are executing the kernel on
							* A program is represented by the **program** class
						</div>
					</div>
				</section>
				<!--Slide 32-->
				<section>
										<div class="container">
						<div class="col-left-3">
							<code><pre>
int main(int argc, char *argv[]) {
  std::vector&ltfloat&gt dA{ … }, dB{ … }, dO{ … };
  try { 
    queue q(gpu_selector{}, 
      async_handler{});

    program addProgram(q.get_context());
    <mark>addProgram.build_with_kernel_type&ltadd&gt();</mark>

    buffer&ltfloat, 1&gt bufA(dA.data(), range&lt1&gt(dA.size()));
    buffer&ltfloat, 1&gt bufB(dB.data(), range&lt1&gt(dB.size()));
    buffer&ltfloat, 1&gt bufO(dO.data(), range&lt1&gt(dO.size()));
    q.submit([&](handler &cgh){

      auto inA = bufA.get_access&ltaccess::mode::read&gt(cgh);
      auto inB = bufB.get_access&ltaccess::mode::read&gt(cgh);
      auto out = bufO.get_access&ltaccess::mode::write&gt(cgh);

      cgh.parallel_for(range&lt1&gt(dA.size()), 
        add{inA, inB, out});
    }); 
    q.wait_and_throw();
  } catch (...) { /* handle errors */ }
}
							</code></pre>
						</div>
						<div class="col-right-2" data-markdown>
							* The program can be then be compiled from a SYCL kernel function name by calling **compile_with_kernel_type** and specifying the kernel name as a template parameter
							* If the kernel function was defined as a function object then the name would be the function object type
						</div>
					</div>
				</section>
				<!--Slide 33-->
				<section>
										<div class="container">
						<div class="col-left-3">
							<code><pre>
int main(int argc, char *argv[]) {
  std::vector&ltfloat&gt dA{ … }, dB{ … }, dO{ … };
  try { 
    queue q(gpu_selector{}, async_handler{});

    program addProgram(q.get_context());
    addProgram.build_with_kernel_type&ltadd&gt();
    <mark>auto addKernel = addProgram.get_kernel&ltadd&gt();</mark>

    buffer&ltfloat, 1&gt bufA(dA.data(), range&lt1&gt(dA.size()));
    buffer&ltfloat, 1&gt bufB(dB.data(), range&lt1&gt(dB.size()));
    buffer&gtfloat, 1&lt bufO(dO.data(), range&lt1&gt(dO.size()));
    q.submit([&](handler &cgh){

      auto inA = bufA.get_access&ltaccess::mode::read&gt(cgh);
      auto inB = bufB.get_access&ltaccess::mode::read&gt(cgh);
      auto out = bufO.get_access&ltaccess::mode::write&gt(cgh);

      cgh.parallel_for(range&lt1&gt(dA.size()), 
	    add{inA, inB, out});
    }); 
    q.wait_and_throw();
  } catch (...) { /* handle errors */ }
}
							</code></pre>
						</div>
						<div class="col-right-2" data-markdown>
							* Once the program is compiled you can retrieve the kernel
							* A kernel is represented by the **kernel** class
							* This is retrieved from a program object by calling the **get_kernel** member function and specifying the kernel name as a template parameter
						</div>
					</div>
				</section>
				<!--Slide 34-->
				<section>
										<div class="container">
						<div class="col-left-3">
							<code><pre>
int main(int argc, char *argv[]) {
  std::vector&ltfloat&gt dA{ … }, dB{ … }, dO{ … };
  try{
    queue q(gpu_selector{}, async_handler{});

    program addProgram(q.get_context());
    addProgram.build_with_kernel_type&ltadd&gt();
    auto addKernel = addProgram.get_kernel&ltadd&gt();

    buffer&ltfloat, 1&gt bufA(dA.data(), range&lt1&gt(dA.size()));
    buffer&ltfloat, 1&gt bufB(dB.data(), range&lt1&gt(dB.size()));
    buffer&ltfloat, 1&gt bufO(dO.data(), range&lt1&gt(dO.size()));
    q.submit([&](handler &cgh){

      auto inA = bufA.get_access&ltaccess::mode::read&gt(cgh);
      auto inB = bufB.get_access&ltaccess::mode::read&gt(cgh);
      auto out = bufO.get_access&ltaccess::mode::write&gt(cgh);

      cgh.parallel_for(<mark>addKernel</mark>,
       range&lt1&gt(dA.size()), [=](id&lt1&gt i)
       { out[i] = inA[i] + inB[i]; }); 
    }); 
    q.wait_and_throw();
  } catch (...) { /* handle errors */ }
}
							</code></pre>
						</div>
						<div class="col-right-2" data-markdown>
							* Finally the kernel object can then be passed to parallel_for in order to specify that the invocation should use the precompiled kernel
						</div>
					</div>
				</section>
				<!--Slide 35-->
				<section class="hbox" data-markdown>
					## Questions
				</section>
			</div>
		</div>
		<script src="../common-revealjs/js/reveal.js"></script>
		<script src="../common-revealjs/plugin/markdown/marked.js"></script>
		<script src="../common-revealjs/plugin/markdown/markdown.js"></script>
		<script src="../common-revealjs/plugin/notes/notes.js"></script>
		<script>
	Reveal.initialize();
	Reveal.configure({ slideNumber: true });
		</script>
	</body>
</html>
